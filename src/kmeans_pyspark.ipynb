{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCM0YMSO38X3",
        "outputId": "6648241c-fb40-4197-9782-8468e012a36b"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVyJYg4h4Ect",
        "outputId": "d75860f3-a26e-42ab-cb2f-d12f275e8f7b"
      },
      "outputs": [],
      "source": [
        "# !pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsgVYvd64GaK",
        "outputId": "586a4e42-b1e8-4449-d66c-350e682d5434"
      },
      "outputs": [],
      "source": [
        "# !git clone https://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:@github.com/BasmaElhoseny01/Big-Data-Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "HXs-Yv_K4REH"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "import sys\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "yFPhXIsu4VP9"
      },
      "outputs": [],
      "source": [
        "# check spark installation\n",
        "findspark.init()\n",
        "\n",
        "# Create Spark Session\n",
        "spark=SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"KmeansClustering\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create Spark Context\n",
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "P_XPuyR14XnZ"
      },
      "outputs": [],
      "source": [
        "# Read Input Data\n",
        "# [FIX] Split Step\n",
        "# data_rdd = sc.textFile(\"/content/Big-Data-Project/3d_10000n_7k.txt\")\n",
        "data_rdd = sc.textFile(\"3d_10000n_7k.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL5RxUdD4ZoI",
        "outputId": "a3345e31-3cf5-48a9-9a6d-44652d55463e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['-1.0472979515228584,5.231936223538009,6.608318888642993', '-10.522536074590798,0.7698579117624849,4.324456527058799', '-8.113011201832247,2.5982909987071263,-9.462184807455044']\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "# # Show first 10 rows\n",
        "print(data_rdd.take(3))\n",
        "print(data_rdd.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "yTggM93T4bYP"
      },
      "outputs": [],
      "source": [
        "K=5\n",
        "maxIterations = 15\n",
        "distance_threshold= 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "1CCcl_sO4c4Z"
      },
      "outputs": [],
      "source": [
        "#CENTROIDS CONVERSION\n",
        "centroids=[]\n",
        "\n",
        "tmp = [line.split(\",\") for line in data_rdd.takeSample(False, K)]\n",
        "for index, centroid in enumerate(tmp):\n",
        "    centroids += [[index, [float(string) for string in centroid]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqJprYnT4eXG",
        "outputId": "54d698b4-9888-486d-95bc-cc3487e28c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[-1.0472979515228584, 5.231936223538009, 6.608318888642993], 1], [[-10.522536074590798, 0.7698579117624849, 4.324456527058799], 1], [[-8.113011201832247, 2.5982909987071263, -9.462184807455044], 1]]\n"
          ]
        }
      ],
      "source": [
        "##POINTS CONVERSION\n",
        "points_rdd = data_rdd.map(lambda line: [[float(string) for string in line.split(',')], 1])\n",
        "points_rdd.cache()\n",
        "\n",
        "print(points_rdd.take(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "GTcYTPN44gM-"
      },
      "outputs": [],
      "source": [
        "def get_closest_centroid(point, centroids):\n",
        "    '''\n",
        "    Function to get the closest centroid to a point\n",
        "    point: list of float [x1, x2, x3, ..., xn]\n",
        "    centroids: list of list [[index, [x1, x2, x3, ..., xn]], ...]\n",
        "\n",
        "    return: closest centroid list [index, [x1, x2, x3, ..., xn]]\n",
        "    '''\n",
        "    closest_centroid = centroids[0] # Closest centroid Index\n",
        "    closest_distance = float('inf') # distance between point and closest centroid\n",
        "    for centroid in centroids:\n",
        "        # Compute Euclidean distance between point and centroid rule: sqrt(sum((a-b)^2))\n",
        "        distance = sum([(a - b) ** 2 for a, b in zip(point, centroid[1])]) ** 0.5\n",
        "\n",
        "        if distance < closest_distance:\n",
        "            closest_distance = distance\n",
        "            closest_centroid = centroid\n",
        "    return closest_centroid[0]\n",
        "\n",
        "def sum_2_points(p1, p2):\n",
        "    '''\n",
        "    Function to sum 2 points\n",
        "    P1: tuple of 2 elements points and count of points (point(added), no_points)\n",
        "    P2: tuple of 2 elements points and count of points (point(added), no_points)\n",
        "    '''\n",
        "    # Element-wise Summation of the 2 points\n",
        "    # Apply the square function to each element of the list using map\n",
        "    # points_sum = list(map(sum,p1[0], p2[0]))\n",
        "    # points_sum = [0.5,0.6,0.9]\n",
        "    points_sum = [x + y for x, y in zip(p1[0], p2[0])]\n",
        "\n",
        "    # Increment the total number of points\n",
        "    points_counter = p1[1] + p2[1]\n",
        "\n",
        "    # Return the sum of the 2 points and the sum of the counts\n",
        "    return [points_sum, points_counter]\n",
        "\n",
        "def average_points(p):\n",
        "  '''\n",
        "  p: tuple of 2 elements points and count of points (point(added), no_points)\n",
        "  '''\n",
        "\n",
        "  return list(map( lambda x: x / p[1], p[0]))\n",
        "\n",
        "def ecludien_dist(p1,p2):\n",
        "  '''\n",
        "  '''\n",
        "  squared_differences = [(x - y) ** 2 for x, y in zip(p1, p2)]\n",
        "  distance_squared = sum(squared_differences)\n",
        "  return math.sqrt(distance_squared)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7qQL5SK4h7f",
        "outputId": "c51e4abe-1589-4b0d-d33f-d4cb4e91bfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 1\n",
            "Iteration: 2\n",
            "Iteration: 3\n",
            "Iteration: 4\n",
            "Iteration: 5\n",
            "Iteration: 6\n",
            "Iteration: 7\n",
            "Iteration: 8\n",
            "Iteration: 9\n",
            "Iteration: 10\n",
            "Iteration: 11\n",
            "Iteration: 12\n",
            "Iteration: 13\n",
            "Iteration: 14\n",
            "Iteration: 15\n"
          ]
        }
      ],
      "source": [
        "iterations = 0\n",
        "while(maxIterations > iterations):\n",
        "    iterations += 1\n",
        "    print(\"Iteration: \" + str(iterations))\n",
        "\n",
        "\n",
        "\n",
        "    # (1) Mapper Compute the closest centroid for each point :D\n",
        "    # input is a point, output is a tuple with the index of the closest centroid and the point itself\n",
        "    # (P,1) -> (i, P) : i is the index of the closest centroid to P\n",
        "    closest_centroids_rdd = points_rdd.map(lambda point: (get_closest_centroid(point[0], centroids), point))\n",
        "    # print(closest_centroids_rdd.take(10))\n",
        "\n",
        "    # (2) Combine the points that belong to the same centroid (Partial Sum)   [Per Machine]\n",
        "    # input is a tuple (i, P), output is a tuple (i, P1+P2+...+Pn)\n",
        "    combined_points_rdd = closest_centroids_rdd.reduceByKey(lambda p1, p2 : sum_2_points(p1, p2)) #  In the reduceByKey operation, the lambda function is applied iteratively to pairs of values with the same key. If you have three points with the same key, the lambda function will be applied to the first two points, then the result of that operation will be combined with the third point, and so on.\n",
        "    # print(combined_points_rdd.collect())\n",
        "\n",
        "    # (3) Shuffle and sort [Not Needed]\n",
        "\n",
        "    # (4) Reducer Compute the new centroids\n",
        "    centroids_rdd=combined_points_rdd.mapValues(lambda centroid: average_points(centroid)).sortByKey(ascending=True)\n",
        "\n",
        "    new_centroids=centroids_rdd.collect()\n",
        "    # print(centroids)\n",
        "\n",
        "    # Check Convergence\n",
        "    convergedCentroids = 0\n",
        "    for centroid in centroids:\n",
        "      centroid_index=centroid[0]\n",
        "      distance=ecludien_dist(centroid[1],new_centroids[centroid_index][1])\n",
        "\n",
        "      if distance<distance_threshold:\n",
        "        convergedCentroids+=1\n",
        "\n",
        "\n",
        "    centroids=new_centroids\n",
        "\n",
        "    # If no of converged centroids is more then 80% the  done\n",
        "    if convergedCentroids > len(centroids)*80/100:\n",
        "            print(\"Centroids converged\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lw12cZp4vXR",
        "outputId": "903ad350-97db-4e0b-872a-cb865e59a4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, [8.540660063738132, 3.797135494664097, -10.016533581717015]), (1, [-9.023399286354179, -0.9239452138252954, 3.6051181686516967]), (2, [-3.890863163070286, -4.5981781188539825, 3.5875857472576986]), (3, [-8.46228786359154, 4.580668105683215, -9.634251258371473]), (4, [0.16452726549643057, 6.102692624340226, 5.320165920664669])]\n"
          ]
        }
      ],
      "source": [
        "# Stop the SparkContext in Apache Spark.\n",
        "print(centroids)\n",
        "# sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Centroids\n",
        "with open('models/centroids.txt', 'w') as f:\n",
        "    for centroid in centroids:\n",
        "        f.write(','.join([str(x) for x in centroid[1]]) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Assign each data point to its nearest centroid\n",
        "nearest_centroids_rdd = points_rdd.map(lambda x: (get_closest_centroid(x[0], centroids), x[0]))\n",
        "# print(nearest_centroids_rdd.take(3))\n",
        "print(nearest_centroids_rdd.count())\n",
        "\n",
        "# Compute the Silhouette Coefficient for each data point\n",
        "def silhouette_coefficient(point, nearest_centroid, other_points_in_cluster):\n",
        "    a = sum(Vectors.squared_distance(point, p) for p in other_points_in_cluster) / len(other_points_in_cluster)\n",
        "    b = Vectors.squared_distance(point, nearest_centroid)\n",
        "    return (b - a) / max(a, b)\n",
        "\n",
        "# silhouette_scores_rdd = nearest_centroids_rdd.map(lambda x: silhouette_coefficient(x[0], x[1], nearest_centroids_rdd.filter(lambda y: y[0] == x[0]).map(lambda z: z[1]).collect()))\n",
        "\n",
        "# Compute the average Silhouette Score\n",
        "average_silhouette_score = silhouette_scores_rdd.mean()\n",
        "print(\"Average Silhouette Score:\", average_silhouette_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "# Compute shilouette score\n",
        "# Compute the average distance between each point and all other points in the same cluster\n",
        "# Compute the average distance between each point and all other points in the next nearest cluster\n",
        "# Compute the silhouette score for each point\n",
        "# Compute the average silhouette score for all points\n",
        "# The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
