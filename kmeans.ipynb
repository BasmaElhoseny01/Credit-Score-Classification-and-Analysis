{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCM0YMSO38X3",
        "outputId": "6648241c-fb40-4197-9782-8468e012a36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVyJYg4h4Ect",
        "outputId": "d75860f3-a26e-42ab-cb2f-d12f275e8f7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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:@github.com/BasmaElhoseny01/Big-Data-Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsgVYvd64GaK",
        "outputId": "586a4e42-b1e8-4449-d66c-350e682d5434"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Big-Data-Project'...\n",
            "warning: redirecting to https://github.com/BasmaElhoseny01/Big-Data-Project/\n",
            "remote: Enumerating objects: 315, done.\u001b[K\n",
            "remote: Counting objects: 100% (278/278), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 315 (delta 134), reused 222 (delta 78), pack-reused 37\u001b[K\n",
            "Receiving objects: 100% (315/315), 62.83 MiB | 22.03 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "Updating files: 100% (85/85), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "import sys\n",
        "import math"
      ],
      "metadata": {
        "id": "HXs-Yv_K4REH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check spark installation\n",
        "findspark.init()\n",
        "\n",
        "# Create Spark Session\n",
        "spark=SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"KmeansClustering\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create Spark Context\n",
        "sc=spark.sparkContext"
      ],
      "metadata": {
        "id": "yFPhXIsu4VP9"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Input Data\n",
        "# [FIX] Split Step\n",
        "data_rdd = sc.textFile(\"/content/Big-Data-Project/3d_10000n_7k.txt\")"
      ],
      "metadata": {
        "id": "P_XPuyR14XnZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Show first 10 rows\n",
        "print(data_rdd.take(3))\n",
        "print(data_rdd.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL5RxUdD4ZoI",
        "outputId": "a3345e31-3cf5-48a9-9a6d-44652d55463e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['-1.0472979515228584,5.231936223538009,6.608318888642993', '-10.522536074590798,0.7698579117624849,4.324456527058799', '-8.113011201832247,2.5982909987071263,-9.462184807455044']\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K=5\n",
        "maxIterations = 100\n",
        "distance_threshold= 0.05"
      ],
      "metadata": {
        "id": "yTggM93T4bYP"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CENTROIDS CONVERSION\n",
        "centroids=[]\n",
        "\n",
        "tmp = [line.split(\",\") for line in data_rdd.takeSample(False, K)]\n",
        "for index, centroid in enumerate(tmp):\n",
        "    centroids += [[index, [float(string) for string in centroid]]]"
      ],
      "metadata": {
        "id": "1CCcl_sO4c4Z"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##POINTS CONVERSION\n",
        "points_rdd = data_rdd.map(lambda line: [[float(string) for string in line.split(',')], 1])\n",
        "# points_rdd.cache()\n",
        "print(points_rdd.take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqJprYnT4eXG",
        "outputId": "54d698b4-9888-486d-95bc-cc3487e28c2b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-1.0472979515228584, 5.231936223538009, 6.608318888642993], 1], [[-10.522536074590798, 0.7698579117624849, 4.324456527058799], 1], [[-8.113011201832247, 2.5982909987071263, -9.462184807455044], 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_closest_centroid(point, centroids):\n",
        "    '''\n",
        "    Function to get the closest centroid to a point\n",
        "    point: list of float [x1, x2, x3, ..., xn]\n",
        "    centroids: list of list [[index, [x1, x2, x3, ..., xn]], ...]\n",
        "\n",
        "    return: closest centroid list [index, [x1, x2, x3, ..., xn]]\n",
        "    '''\n",
        "    closest_centroid = centroids[0] # Closest centroid Index\n",
        "    closest_distance = float('inf') # distance between point and closest centroid\n",
        "    for centroid in centroids:\n",
        "        # Compute Euclidean distance between point and centroid rule: sqrt(sum((a-b)^2))\n",
        "        distance = sum([(a - b) ** 2 for a, b in zip(point, centroid[1])]) ** 0.5\n",
        "\n",
        "        if distance < closest_distance:\n",
        "            closest_distance = distance\n",
        "            closest_centroid = centroid\n",
        "    return closest_centroid[0]\n",
        "\n",
        "def sum_2_points(p1, p2):\n",
        "    '''\n",
        "    Function to sum 2 points\n",
        "    P1: tuple of 2 elements points and count of points (point(added), no_points)\n",
        "    P2: tuple of 2 elements points and count of points (point(added), no_points)\n",
        "    '''\n",
        "    # Element-wise Summation of the 2 points\n",
        "    # Apply the square function to each element of the list using map\n",
        "    # points_sum = list(map(sum,p1[0], p2[0]))\n",
        "    # points_sum = [0.5,0.6,0.9]\n",
        "    points_sum = [x + y for x, y in zip(p1[0], p2[0])]\n",
        "\n",
        "    # Increment the total number of points\n",
        "    points_counter = p1[1] + p2[1]\n",
        "\n",
        "    # Return the sum of the 2 points and the sum of the counts\n",
        "    return [points_sum, points_counter]\n",
        "\n",
        "def average_points(p):\n",
        "  '''\n",
        "  p: tuple of 2 elements points and count of points (point(added), no_points)\n",
        "  '''\n",
        "\n",
        "  return list(map( lambda x: x / p[1], p[0]))\n",
        "\n",
        "def ecludien_dist(p1,p2):\n",
        "  '''\n",
        "  '''\n",
        "  squared_differences = [(x - y) ** 2 for x, y in zip(p1, p2)]\n",
        "  distance_squared = sum(squared_differences)\n",
        "  return math.sqrt(distance_squared)\n",
        "\n"
      ],
      "metadata": {
        "id": "GTcYTPN44gM-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 0\n",
        "while(maxIterations > iterations):\n",
        "    iterations += 1\n",
        "    print(\"Iteration: \" + str(iterations))\n",
        "\n",
        "\n",
        "\n",
        "    # (1) Mapper Compute the closest centroid for each point :D\n",
        "    # input is a point, output is a tuple with the index of the closest centroid and the point itself\n",
        "    # (P,1) -> (i, P) : i is the index of the closest centroid to P\n",
        "    closest_centroids_rdd = points_rdd.map(lambda point: (get_closest_centroid(point[0], centroids), point))\n",
        "    # print(closest_centroids_rdd.take(10))\n",
        "\n",
        "    # (2) Combine the points that belong to the same centroid (Partial Sum)   [Per Machine]\n",
        "    # input is a tuple (i, P), output is a tuple (i, P1+P2+...+Pn)\n",
        "    combined_points_rdd = closest_centroids_rdd.reduceByKey(lambda p1, p2 : sum_2_points(p1, p2)) #  In the reduceByKey operation, the lambda function is applied iteratively to pairs of values with the same key. If you have three points with the same key, the lambda function will be applied to the first two points, then the result of that operation will be combined with the third point, and so on.\n",
        "    # print(combined_points_rdd.collect())\n",
        "\n",
        "\n",
        "    # (3) Shuffle and sort\n",
        "\n",
        "    # (4) Reducer Compute the new centroids\n",
        "    centroids_rdd=combined_points_rdd.mapValues(lambda centroid: average_points(centroid)).sortByKey(ascending=True)\n",
        "\n",
        "    new_centroids=centroids_rdd.collect()\n",
        "    # print(centroids)\n",
        "\n",
        "    # Check Convergence\n",
        "    convergedCentroids = 0\n",
        "    for centroid in centroids:\n",
        "      centroid_index=centroid[0]\n",
        "      distance=ecludien_dist(centroid[1],new_centroids[centroid_index][1])\n",
        "\n",
        "      if distance<distance_threshold:\n",
        "        convergedCentroids+=1\n",
        "\n",
        "\n",
        "    centroids=new_centroids\n",
        "\n",
        "    # If no of converged centroids is more then 80% the  done\n",
        "    if convergedCentroids > len(centroids)*80/100:\n",
        "            print(\"Centroids converged\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7qQL5SK4h7f",
        "outputId": "c51e4abe-1589-4b0d-d33f-d4cb4e91bfb1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Iteration: 2\n",
            "Iteration: 3\n",
            "Iteration: 4\n",
            "Centroids converged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Stop the SparkContext in Apache Spark.\n",
        "print(centroids)\n",
        "# sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lw12cZp4vXR",
        "outputId": "903ad350-97db-4e0b-872a-cb865e59a4c1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, [-5.494432392594969, -4.25835601283858, 2.7226809942056303]), (1, [0.19025389537675313, 6.116112140359886, 5.317616244640591]), (2, [-8.46228786359154, 4.580668105683215, -9.634251258371473]), (3, [8.540660063738132, 3.797135494664097, -10.016533581717015]), (4, [-9.511885749667574, 1.8421230334847665, 6.192046367292705])]\n"
          ]
        }
      ]
    }
  ]
}