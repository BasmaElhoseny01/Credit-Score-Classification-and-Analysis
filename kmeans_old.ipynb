{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b195c124-b282-40db-be50-2c6ac2fb432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Test PySpark\n",
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e008f9-642d-4c74-8637-b67ef5e15635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\n",
      "root\n",
      " |-- Age: float (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- Annual_Income: float (nullable = true)\n",
      " |-- Monthly_Inhand_Salary: float (nullable = true)\n",
      " |-- Num_Bank_Accounts: float (nullable = true)\n",
      " |-- Num_Credit_Card: float (nullable = true)\n",
      " |-- Interest_Rate: float (nullable = true)\n",
      " |-- Num_of_Loan: float (nullable = true)\n",
      " |-- Delay_from_due_date: float (nullable = true)\n",
      " |-- Num_of_Delayed_Payment: float (nullable = true)\n",
      " |-- Changed_Credit_Limit: float (nullable = true)\n",
      " |-- Num_Credit_Inquiries: float (nullable = true)\n",
      " |-- Credit_Mix: string (nullable = true)\n",
      " |-- Outstanding_Debt: float (nullable = true)\n",
      " |-- Credit_Utilization_Ratio: float (nullable = true)\n",
      " |-- Payment_of_Min_Amount: string (nullable = true)\n",
      " |-- Total_EMI_per_month: float (nullable = true)\n",
      " |-- Amount_invested_monthly: float (nullable = true)\n",
      " |-- Payment_Behaviour: string (nullable = true)\n",
      " |-- Monthly_Balance: float (nullable = true)\n",
      " |-- Credit_Score: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "Cluster Centers: \n",
      "[3.32701887e+01 5.28752331e+04 4.19796222e+03 5.36806319e+00\n",
      " 5.53216581e+00 1.45325290e+01 3.53326541e+00 2.10878772e+01\n",
      " 3.08827161e+01 1.03850460e+01 2.72885289e+01 1.42602166e+03\n",
      " 3.22828429e+01 1.40320070e+03 6.38938117e+02 4.03115683e+02]\n",
      "[ 3.52222222e+01  5.32248202e+04  4.48762387e+03  4.00000000e+00\n",
      "  5.22222222e+00  1.11111111e+01  3.22222222e+00  1.41111111e+01\n",
      "  9.66666667e+00  1.19133335e+01  6.66666667e+00  1.25974888e+03\n",
      "  3.45859985e+01  1.62995905e+03  2.51965769e+02 -3.33333329e+26]\n",
      "[3.40221519e+01 2.05161810e+07 4.55668766e+03 5.20886076e+00\n",
      " 5.63291139e+00 1.44873418e+01 3.58227848e+00 1.96550633e+01\n",
      " 3.20031646e+01 1.04410126e+01 3.56424051e+01 1.44046399e+03\n",
      " 3.27398094e+01 1.35339945e+03 4.28245476e+02 4.15079866e+02]\n",
      "[3.36960000e+01 7.10033065e+06 4.06244233e+03 5.16400000e+00\n",
      " 5.57200000e+00 1.40680000e+01 3.24000000e+00 2.05240000e+01\n",
      " 1.30920000e+01 1.00198457e+01 2.40960000e+01 1.36502560e+03\n",
      " 3.24042558e+01 1.87570001e+03 4.22200392e+02 3.97172897e+02]\n",
      "[3.35234899e+01 1.38977584e+07 4.09242589e+03 5.56711409e+00\n",
      " 5.58724832e+00 1.49228188e+01 3.60738255e+00 2.12885906e+01\n",
      " 4.22718121e+01 1.17573634e+01 3.26442953e+01 1.53358406e+03\n",
      " 3.24086124e+01 1.02508993e+03 5.96666292e+02 3.96129655e+02]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeansExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # Load your dataset as a DataFrame\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dataset/train_preprocessed.csv\")\n",
    "\n",
    "# # Assuming your data has features in columns \"feature1\", \"feature2\", ..., \"featureN\"\n",
    "# # You need to assemble these features into a single vector column for KMeans\n",
    "feature_cols = [ 'Age',\n",
    "        'Annual_Income',\n",
    "        'Monthly_Inhand_Salary', \n",
    "        'Num_Bank_Accounts',\n",
    "        'Num_Credit_Card',\n",
    "        'Interest_Rate',\n",
    "        'Num_of_Loan',\n",
    "        'Delay_from_due_date',\n",
    "        'Num_of_Delayed_Payment',\n",
    "        'Changed_Credit_Limit',\n",
    "        'Num_Credit_Inquiries',\n",
    "        'Outstanding_Debt',\n",
    "        'Credit_Utilization_Ratio',\n",
    "        'Total_EMI_per_month',\n",
    "        'Amount_invested_monthly',\n",
    "        'Monthly_Balance']\n",
    "\n",
    "# Define the target data type\n",
    "target_data_type = \"float\"\n",
    "\n",
    "# Iterate through string columns and cast them to the target data type\n",
    "for column in feature_cols:\n",
    "    data = data.withColumn(column, col(column).cast(target_data_type))\n",
    "    \n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Print DataFrame schema\n",
    "print(\"DataFrame Schema:\")\n",
    "data.printSchema()\n",
    "\n",
    "# # Print a sample of DataFrame contents\n",
    "# print(\"Sample of DataFrame contents:\")\n",
    "# data.show(5)\n",
    "\n",
    "# # Trains a KMeans model\n",
    "kmeans = KMeans().setK(5).setSeed(1)  # Set the number of clusters (k) and seed for reproducibility\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# # Make predictions\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# # Shows the result\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d49566-122e-4165-a495-8b896251b88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
